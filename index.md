## Introduction
This resource is intended to help practitioners and project managers working in disaster risk ensure that the deployment of artificial intelligence (AI), and machine learning (ML) technologies in particular, is done in a manner that is both effective and responsible. The content was produced over a 6 month interdisciplinary collaboration between experts from intergovernmental organizations, non-profits, academia, and the private sector. We are releasing it here in draft form in order to generate further discussion and gain additional feedback from our intended audience. It is our hope that, as a product produced through intensive consultation with the community who it is written for, that this resource will inform and improve the important work carried about by data scientists, risk modelers, and other technical experts working in disaster risk management (DRM).  

Our community continues to explore the opportunities offered by machine learning technologies and expand the range of applications for which they are used. While we welcome the potential of these tools, we need to pay close attention to the significant risks that unconsidered deployment of these tools in the extremely complex contexts in which DRM is undertaken may create. Significant attention is currently being given by academics, journalists, and the public to questions of the ethics and bias of AI systems across a variety of domains including facial recognition (Keyes 2018), automated weaponry (Suchman et al 2016), search (Noble 2018), and criminal justice (Eubanks 2018). Despite significant potential for negative impacts of these tools in disaster risk management, our domain has not given these issues as much attention as other domains. 

Some example of specific issues that might be concerns as a direct result of improper use of machine learning include:


* Perpetuating and aggravating societal inequalities through use of biased datasets
* Aggravating privacy and security concerns in Fragility Conflict and Violence settings through combination of previously distinct datasets
* Limiting opportunities for public participation in disaster risk management due to increased complexity of data products
* Reducing the role of expert judgement is data and modeling tasks in turn increasing probability of error or misuse.  
* Hype, and availability of private sector funding for artificial intelligence technology in DRM, leads to overstatement of the capacities of these tools and the deployment of untested approaches in safety-critical scenarios

These are risks that need to be weighed seriously against the potential benefits before introducing new technologies into disaster information systems. While there are some relevant guidelines being produced in adjacent domains, the conversation is still evolving. In some cases, like facial recognition, experts have begun to recommend not using it all and they have been banned in a number of jurisdictions. It is too early to know how this debate will play out in the field of disaster risk management so it is worth proceeding with caution.

There are many ways by which, as we will discuss, through technical means as well as improved project design and management, machine learning projects can be more responsibly developed and applied. However, we will also note some of these issues go deeper than machine learning, and are rooted in how we collect disaster risk and impact data, and how we design DRM projects more broadly. Here the heightened focus on the social impacts of AI tools offer us an opportunity to draw attention to some of these questions. We address this in Section 4.

To develop this resource, researchers from the World Bank Global Facility for Disaster Reduction and Recovery (GFDRR), Columbia University, and Deltares convened 7 online meetings between January and March of 2020, where machine learning experts and other researchers working in the area of disaster data discussed the opportunities offered by machine learning tools in disaster risk management, potential risks raised by these tools, and opportunities for mitigation. On average, 17 individuals participated in each 90-minute session. These interdisciplinary conversations were shaped by joint readings of relevant research and presentation of detailed case studies. In addition, the project team conducted 14 in-depth interviews with data scientists working on these topics for their views. 

We present the preliminary findings of this process here, for wider review by the community. We have organized the body of the work around 4 key sources of threat: bias, privacy and security risks, lack of transparency and explainability, and hype. For each, we have presented an overview of the topic, realistic threat models along with hypothetical examples, strategies for managing these risks, and suggested further reading. Wherever possible, we keep the text here short and provide plenty of footnotes and links to more information. Over the next 3 months, we will publicize and present this resource for further iteration and review. Through this process, we sought to produce these recommendations collectively as members of the community of expert researchers and practitioners working to create and use disaster data effectively and responsibly.



